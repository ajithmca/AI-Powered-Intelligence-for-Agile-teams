
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd17095",
   "metadata": {
    "papermill": {
     "duration": 0.00961,
     "end_time": "2024-02-05T12:43:34.480839",
     "exception": false,
     "start_time": "2024-02-05T12:43:34.471229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AI-Powered Intelligence for Agile Teams\n",
    "### 1. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09453388",
   "metadata": {
    "papermill": {
     "duration": 1.393499,
     "end_time": "2024-02-05T12:43:35.882801",
     "exception": false,
     "start_time": "2024-02-05T12:43:34.489302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sentistrength import PySentiStr\n",
    "import re\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0e830-f3dd-4534-8894-44094dadb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzers\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "sentistrength = PySentiStr()\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
  
    "# Load configuration from config.yaml\n",
    "# Determine project root based on current working directory\n",
    "# If we're in src/, go up one level; otherwise assume we're at project root\n",
    "current_dir = os.getcwd()\n",
    "if os.path.basename(current_dir) == 'src':\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "config_path = os.path.join(project_root, 'config', 'config.yaml')\n",
    "\n",
    "# Verify config file exists\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Config file not found at: {config_path}\\n\"\n",
    "        f\"Current working directory: {current_dir}\\n\"\n",
    "        f\"Project root: {project_root}\\n\"\n",
    "        f\"Please ensure you're running the notebook from the project root or src/ directory.\"\n",
    "    )\n",
    "\n",
    "# Read config file\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract data file path from config\n",
    "data_file_path = config['data']['file_path']\n",
    "data_file_type = config['data'].get('file_type', 'auto')\n",
    "\n",
    "# Resolve data file path relative to project root\n",
    "if not os.path.isabs(data_file_path):\n",
    "    # Handle relative paths (e.g., \"../data/file.xlsx\" or \"data/file.xlsx\")\n",
    "    # Remove \"../\" prefix and join to project root\n",
    "    path_clean = data_file_path.replace('../', '') if data_file_path.startswith('../') else data_file_path\n",
    "    data_file_path = os.path.join(project_root, path_clean)\n",
    "\n",
    "# Verify data file exists\n",
    "if not os.path.exists(data_file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data file not found at: {data_file_path}\\n\"\n",
    "        f\"Please check the 'file_path' setting in config.yaml\"\n",
    "    )\n",
    "\n",
    "# Display loaded configuration\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Config file: {config_path}\")\n",
    "print(f\"Data file path: {data_file_path}\")\n",
    "print(f\"Data file type: {data_file_type}\")\n",
    "\n",
    "# Store LLM config for future use\n",
    "llm_config = config.get('llm', {})\n",
    "print(f\"LLM Provider: {llm_config.get('provider', 'none')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8e2dc",
   "metadata": {
    "papermill": {
     "duration": 7.392908,
     "end_time": "2024-02-05T12:43:43.284803",
     "exception": false,
     "start_time": "2024-02-05T12:43:35.891895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the file using path from config\n",
    "# Determine file type if auto-detection is enabled\n",
    "if data_file_type == 'auto':\n",
    "    # Auto-detect from file extension\n",
    "    file_ext = os.path.splitext(data_file_path)[1].lower()\n",
    "    if file_ext in ['.xlsx', '.xls']:\n",
    "        data_file_type = 'excel'\n",
    "    elif file_ext == '.csv':\n",
    "        data_file_type = 'csv'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_ext}. Please specify file_type in config.\")\n",
    "\n",
    "# Read file based on detected/configured type\n",
    "if data_file_type == 'excel':\n",
    "    readfile = pd.read_excel(data_file_path)\n",
    "elif data_file_type == 'csv':\n",
    "    readfile = pd.read_csv(data_file_path)\n",
    "else:\n",
    "    raise ValueError(f\"Invalid file_type in config: {data_file_type}. Must be 'excel', 'csv', or 'auto'.\")\n",
    "\n",
    "print(f\"Successfully loaded data from: {data_file_path}\")\n",
    "print(f\"Data shape: {readfile.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0024d8",
   "metadata": {
    "papermill": {
     "duration": 0.598295,
     "end_time": "2024-02-05T12:43:43.891953",
     "exception": false,
     "start_time": "2024-02-05T12:43:43.293658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "readfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7803aef",
   "metadata": {
    "papermill": {
     "duration": 0.057709,
     "end_time": "2024-02-05T12:43:43.960633",
     "exception": false,
     "start_time": "2024-02-05T12:43:43.902924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "readfile.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0a0bc",
   "metadata": {
    "papermill": {
     "duration": 0.036913,
     "end_time": "2024-02-05T12:43:44.011213",
     "exception": false,
     "start_time": "2024-02-05T12:43:43.974300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(readfile.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79aeef",
   "metadata": {
    "papermill": {
     "duration": 0.042127,
     "end_time": "2024-02-05T12:43:44.067158",
     "exception": false,
     "start_time": "2024-02-05T12:43:44.025031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2e917",
   "metadata": {
    "papermill": {
     "duration": 0.223348,
     "end_time": "2024-02-05T12:43:44.304891",
     "exception": false,
     "start_time": "2024-02-05T12:43:44.081543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new = readfile[['Issue.id', 'Issue.Type', 'Priority', 'Project.type', 'Status', 'Created', 'Resolution', 'Resolved', 'Component.s', 'Custom.field..Symptom.Severity.', 'Custom.field..Company.', 'Custom.field..Date.of.First.Response.', 'Votes', 'Environment', 'Custom.field..Original.story.points.']].copy()\n",
    "new = readfile[['Summary', 'Issue key', 'Issue Type', 'Status', 'Created', 'Resolved', 'Comment', 'Comment.1', 'Comment.2', 'Comment.3', 'Comment.4', 'Comment.5', 'Comment.6', 'Comment.7', 'Comment.8', 'Comment.9', 'Comment.10', 'Comment.11', 'Comment.12', 'Comment.13', 'Comment.14', 'Comment.15', 'Comment.16', 'Comment.17', 'Comment.18', 'Comment.19', 'Comment.20', 'Comment.21', 'Comment.22', 'Comment.23', 'Comment.24', 'Comment.25', 'Comment.26', 'Comment.27', 'Comment.28', 'Comment.29', 'Comment.30', 'Comment.31', 'Comment.32', 'Comment.33', 'Comment.34', 'Comment.35', 'Comment.36', 'Comment.37', 'Comment.38', 'Comment.39', 'Comment.40', 'Comment.41', 'Comment.42', 'Comment.43', 'Comment.44', 'Comment.45', 'Comment.46', 'Comment.47', 'Comment.48', 'Comment.49', 'Comment.50', 'Comment.51', 'Comment.52', 'Comment.53', 'Comment.54', 'Comment.55', 'Comment.56', 'Comment.57', 'Comment.58', 'Comment.59', 'Comment.60', 'Comment.61', 'Comment.62', 'Comment.63', 'Comment.64', 'Comment.65', 'Comment.66', 'Comment.67', 'Comment.68', 'Comment.69', 'Comment.70', 'Comment.71', 'Comment.72', 'Comment.73', 'Comment.74', 'Comment.75', 'Comment.76', 'Comment.77', 'Comment.78', 'Comment.79', 'Comment.80', 'Comment.81', 'Comment.82', 'Comment.83', 'Comment.84']].copy()\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe27877",
   "metadata": {
    "papermill": {
     "duration": 0.39316,
     "end_time": "2024-02-05T12:43:45.322600",
     "exception": false,
     "start_time": "2024-02-05T12:43:44.929440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "new['Issue Type'].value_counts().plot(kind='bar')\n",
    "plt.title('Issue Type Distribution')\n",
    "plt.xlabel('Issue Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35703ae0",
   "metadata": {},
   "source": [
    "### 2. Sentiment Analysis on Comment Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c6b08",
   "metadata": {},
   "source": [
    "### 4.1 Improved Sentiment Analysis Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3442cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text for better sentiment analysis.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep punctuation that affects sentiment (!, ?, .)\n",
    "    text = re.sub(r'[^\\w\\s!?.]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove timestamps and dates in common formats\n",
    "    text = re.sub(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', '', text)\n",
    "    text = re.sub(r'\\d{1,2}:\\d{2}(?:\\s*[AP]M)?', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove user IDs and hashes (common in JIRA comments)\n",
    "    text = re.sub(r'[a-f0-9]{32,}', '', text)  # Remove long hex strings\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved sentiment analysis functions\n",
    "\n",
    "def analyze_sentiment_textblob(text):\n",
    "    \"\"\"Original TextBlob analysis\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "    \n",
    "    try:\n",
    "        cleaned_text = preprocess_text(text)\n",
    "        if not cleaned_text:\n",
    "            return (np.nan, np.nan, 'neutral')\n",
    "        \n",
    "        blob = TextBlob(cleaned_text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        \n",
    "        if polarity > 0.1:\n",
    "            sentiment_label = 'positive'\n",
    "        elif polarity < -0.1:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return (polarity, subjectivity, sentiment_label)\n",
    "    except:\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"VADER sentiment analysis (better for short texts)\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "    \n",
    "    try:\n",
    "        cleaned_text = preprocess_text(text)\n",
    "        if not cleaned_text:\n",
    "            return (np.nan, np.nan, 'neutral')\n",
    "        \n",
    "        scores = vader_analyzer.polarity_scores(cleaned_text)\n",
    "        polarity = scores['compound']  # VADER compound score ranges from -1 to 1\n",
    "        \n",
    "        # VADER doesn't provide subjectivity, so we use a simple heuristic\n",
    "        subjectivity = (scores['pos'] + scores['neg']) / 2\n",
    "        \n",
    "        # Classify sentiment\n",
    "        if polarity > 0.05:\n",
    "            sentiment_label = 'positive'\n",
    "        elif polarity < -0.05:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return (polarity, subjectivity, sentiment_label)\n",
    "    except:\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "\n",
    "def analyze_sentiment_sentistrength(text):\n",
    "    \"\"\"SentiStrength-SE sentiment analysis (designed for software engineering texts)\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "    \n",
    "    try:\n",
    "        cleaned_text = preprocess_text(text)\n",
    "        if not cleaned_text:\n",
    "            return (np.nan, np.nan, 'neutral')\n",
    "        \n",
    "        # SentiStrength returns positive and negative scores separately\n",
    "        # Format: [positive_score, negative_score] where scores range from 1-5\n",
    "        result = sentistrength.getSentiment(cleaned_text)\n",
    "        \n",
    "        if result and len(result) >= 2:\n",
    "            pos_score = result[0]  # Positive score (1-5)\n",
    "            neg_score = result[1]  # Negative score (1-5)\n",
    "            \n",
    "            # Convert to polarity scale (-1 to 1)\n",
    "            # SentiStrength: 1-5 scale, we normalize to -1 to 1\n",
    "            # Positive sentiment: pos_score > neg_score\n",
    "            # Negative sentiment: neg_score > pos_score\n",
    "            if pos_score > neg_score:\n",
    "                # Positive sentiment: map 1-5 to 0.2-1.0\n",
    "                polarity = (pos_score - 1) / 4.0  # Maps 1->0, 5->1, but we want 0.2-1.0\n",
    "                polarity = 0.2 + (polarity * 0.8)  # Scale to 0.2-1.0\n",
    "            elif neg_score > pos_score:\n",
    "                # Negative sentiment: map 1-5 to -1.0 to -0.2\n",
    "                polarity = -((neg_score - 1) / 4.0)  # Maps 1->0, 5->-1, but we want -1.0 to -0.2\n",
    "                polarity = -0.2 + (polarity * 0.8)  # Scale to -1.0 to -0.2\n",
    "            else:\n",
    "                # Neutral\n",
    "                polarity = 0.0\n",
    "            \n",
    "            # Subjectivity: higher when both scores are away from 1 (neutral)\n",
    "            max_score = max(pos_score, neg_score)\n",
    "            subjectivity = (max_score - 1) / 4.0  # Normalize to 0-1\n",
    "            \n",
    "            # Classify sentiment\n",
    "            if polarity > 0.1:\n",
    "                sentiment_label = 'positive'\n",
    "            elif polarity < -0.1:\n",
    "                sentiment_label = 'negative'\n",
    "            else:\n",
    "                sentiment_label = 'neutral'\n",
    "            \n",
    "            return (polarity, subjectivity, sentiment_label)\n",
    "        else:\n",
    "            return (np.nan, np.nan, 'neutral')\n",
    "    except Exception as e:\n",
    "        # If SentiStrength fails, return neutral\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "\n",
    "def analyze_sentiment_ensemble(text):\n",
    "    \"\"\"\n",
    "    Ensemble method: Combine TextBlob, VADER, and SentiStrength-SE for better accuracy.\n",
    "    Uses weighted average of all available methods.\n",
    "    SentiStrength-SE is specifically designed for software engineering texts (like JIRA comments).\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "    \n",
    "    tb_result = analyze_sentiment_textblob(text)\n",
    "    vd_result = analyze_sentiment_vader(text)\n",
    "    ss_result = analyze_sentiment_sentistrength(text)\n",
    "    \n",
    "    # Collect valid results\n",
    "    valid_results = []\n",
    "    weights = []\n",
    "    \n",
    "    if not pd.isna(tb_result[0]):\n",
    "        valid_results.append(tb_result)\n",
    "        weights.append(0.25)  # TextBlob: 25% weight\n",
    "    \n",
    "    if not pd.isna(vd_result[0]):\n",
    "        valid_results.append(vd_result)\n",
    "        weights.append(0.35)  # VADER: 35% weight (good for short texts)\n",
    "    \n",
    "    if not pd.isna(ss_result[0]):\n",
    "        valid_results.append(ss_result)\n",
    "        weights.append(0.40)  # SentiStrength-SE: 40% weight (designed for SE texts)\n",
    "    \n",
    "    # If no valid results, return neutral\n",
    "    if len(valid_results) == 0:\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "    \n",
    "    # Normalize weights to sum to 1.0\n",
    "    total_weight = sum(weights)\n",
    "    weights = [w / total_weight for w in weights]\n",
    "    \n",
    "    # Weighted combination\n",
    "    combined_polarity = sum(r[0] * w for r, w in zip(valid_results, weights))\n",
    "    combined_subjectivity = sum(r[1] * w for r, w in zip(valid_results, weights))\n",
    "    \n",
    "    # Classify based on combined polarity\n",
    "    if combined_polarity > 0.1:\n",
    "        sentiment_label = 'positive'\n",
    "    elif combined_polarity < -0.1:\n",
    "        sentiment_label = 'negative'\n",
    "    else:\n",
    "        sentiment_label = 'neutral'\n",
    "    \n",
    "    return (combined_polarity, combined_subjectivity, sentiment_label)\n",
    "\n",
    "# Choose analysis method: 'textblob', 'vader', 'sentistrength', or 'ensemble'\n",
    "SENTIMENT_METHOD = 'ensemble'  # Change this to use different methods\n",
    "print(f\"Using sentiment analysis method: {SENTIMENT_METHOD}\")\n",
    "if SENTIMENT_METHOD == 'ensemble':\n",
    "    print(\"Ensemble combines: TextBlob (25%), VADER (35%), SentiStrength-SE (40%)\")\n",
    "\n",
    "# Map method name to function\n",
    "sentiment_methods = {\n",
    "    'textblob': analyze_sentiment_textblob,\n",
    "    'vader': analyze_sentiment_vader,\n",
    "    'sentistrength': analyze_sentiment_sentistrength,\n",
    "    'ensemble': analyze_sentiment_ensemble\n",
    "}\n",
    "\n",
    "analyze_sentiment = sentiment_methods.get(SENTIMENT_METHOD, analyze_sentiment_ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da60862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different sentiment analysis methods on sample comments\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARING SENTIMENT ANALYSIS METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify comment columns first (needed for comparison)\n",
    "comment_columns = [col for col in new.columns if col.startswith('Comment')]\n",
    "\n",
    "# Get some sample comments for comparison\n",
    "sample_comments = []\n",
    "for col in comment_columns[:5]:  # Use first 5 comment columns\n",
    "    non_empty = new[col].dropna()\n",
    "    non_empty = non_empty[non_empty.astype(str).str.strip() != '']\n",
    "    if len(non_empty) > 0:\n",
    "        sample_comments.extend(non_empty.head(3).tolist())\n",
    "    if len(sample_comments) >= 10:\n",
    "        break\n",
    "\n",
    "print(f\"\\nComparing {len(sample_comments)} sample comments:\\n\")\n",
    "\n",
    "comparison_results = []\n",
    "for i, comment in enumerate(sample_comments[:10], 1):\n",
    "    tb = analyze_sentiment_textblob(comment)\n",
    "    vd = analyze_sentiment_vader(comment)\n",
    "    ss = analyze_sentiment_sentistrength(comment)\n",
    "    en = analyze_sentiment_ensemble(comment)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'comment': comment[:100] + '...' if len(comment) > 100 else comment,\n",
    "        'textblob_polarity': tb[0],\n",
    "        'textblob_label': tb[2],\n",
    "        'vader_polarity': vd[0],\n",
    "        'vader_label': vd[2],\n",
    "        'sentistrength_polarity': ss[0],\n",
    "        'sentistrength_label': ss[2],\n",
    "        'ensemble_polarity': en[0],\n",
    "        'ensemble_label': en[2]\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Method Comparison Summary:\")\n",
    "print(f\"TextBlob and VADER agree: {sum(comparison_df['textblob_label'] == comparison_df['vader_label'])}/{len(comparison_df)}\")\n",
    "print(f\"TextBlob and SentiStrength-SE agree: {sum(comparison_df['textblob_label'] == comparison_df['sentistrength_label'])}/{len(comparison_df)}\")\n",
    "print(f\"VADER and SentiStrength-SE agree: {sum(comparison_df['vader_label'] == comparison_df['sentistrength_label'])}/{len(comparison_df)}\")\n",
    "print(f\"TextBlob and Ensemble agree: {sum(comparison_df['textblob_label'] == comparison_df['ensemble_label'])}/{len(comparison_df)}\")\n",
    "print(f\"VADER and Ensemble agree: {sum(comparison_df['vader_label'] == comparison_df['ensemble_label'])}/{len(comparison_df)}\")\n",
    "print(f\"SentiStrength-SE and Ensemble agree: {sum(comparison_df['sentistrength_label'] == comparison_df['ensemble_label'])}/{len(comparison_df)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0062e1",
   "metadata": {},
   "source": [
    "### 4.2 Sentiment Analysis Execution (Using Improved Method)\n",
    "\n",
    "**Improvements Made:**\n",
    "1. **Text Preprocessing**: Removes URLs, emails, timestamps, and noise that can affect sentiment analysis\n",
    "2. **VADER Sentiment Analyzer**: Better for short texts and social media-style comments (common in JIRA)\n",
    "3. **Ensemble Method**: Combines TextBlob and VADER (60% VADER, 40% TextBlob) for improved accuracy\n",
    "4. **Method Selection**: Easy to switch between methods by changing `SENTIMENT_METHOD` variable\n",
    "\n",
    "**To change the method**, edit cell 12 and set:\n",
    "- `SENTIMENT_METHOD = 'textblob'` - Original TextBlob only\n",
    "- `SENTIMENT_METHOD = 'vader'` - VADER only (recommended for short texts)\n",
    "- `SENTIMENT_METHOD = 'ensemble'` - Combined method (recommended for best accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b20b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all comment columns (columns that start with \"Comment\")\n",
    "comment_columns = [col for col in new.columns if col.startswith('Comment')]\n",
    "print(f\"Found {len(comment_columns)} comment columns:\")\n",
    "print(comment_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment of a text\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of a text string.\n",
    "    Returns: (polarity, subjectivity, sentiment_label)\n",
    "    - polarity: ranges from -1 (negative) to 1 (positive)\n",
    "    - subjectivity: ranges from 0 (objective) to 1 (subjective)\n",
    "    - sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == '':\n",
    "        return (np.nan, np.nan, 'neutral')\n",
    "    \n",
    "    try:\n",
    "        blob = TextBlob(str(text))\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        \n",
    "        # Classify sentiment\n",
    "        if polarity > 0.1:\n",
    "            sentiment_label = 'positive'\n",
    "        elif polarity < -0.1:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return (polarity, subjectivity, sentiment_label)\n",
    "    except:\n",
    "        return (np.nan, np.nan, 'neutral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c63463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on all comment columns\n",
    "print(\"Performing sentiment analysis on comment columns...\")\n",
    "sentiment_results = {}\n",
    "new_columns_dict = {}\n",
    "\n",
    "for col in comment_columns:\n",
    "    print(f\"Processing {col}...\")\n",
    "    sentiments = new[col].apply(analyze_sentiment)\n",
    "    \n",
    "    # Extract polarity, subjectivity, and labels and Store results in a dictionary instead of assigning to DataFrame directly\n",
    "    new_columns_dict[f'{col}_polarity'] = sentiments.apply(lambda x: x[0])\n",
    "    new_columns_dict[f'{col}_subjectivity'] = sentiments.apply(lambda x: x[1])\n",
    "    new_columns_dict[f'{col}_sentiment'] = sentiments.apply(lambda x: x[2])\n",
    "    \n",
    "    # Store summary statistics\n",
    "    sentiment_results[col] = {\n",
    "        'total_comments': new[col].notna().sum(),\n",
    "        'non_empty_comments': (new[col].notna() & (new[col].astype(str).str.strip() != '')).sum(),\n",
    "        'avg_polarity': new_columns_dict[f'{col}_polarity'].mean(),\n",
    "        'avg_subjectivity': new_columns_dict[f'{col}_subjectivity'].mean(),\n",
    "        'positive_count': (new_columns_dict[f'{col}_sentiment'] == 'positive').sum(),\n",
    "        'negative_count': (new_columns_dict[f'{col}_sentiment'] == 'negative').sum(),\n",
    "        'neutral_count': (new_columns_dict[f'{col}_sentiment'] == 'neutral').sum()\n",
    "    }\n",
    "\n",
    "# Concatenate all new columns at once to avoid DataFrame fragmentation\n",
    "new = pd.concat([new, pd.DataFrame(new_columns_dict)], axis=1)\n",
    "\n",
    "print(\"Sentiment analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a0a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a summary dataframe for all comment columns\n",
    "sentiment_summary = pd.DataFrame(sentiment_results).T\n",
    "sentiment_summary = sentiment_summary.sort_values('non_empty_comments', ascending=False)\n",
    "print(\"Sentiment Analysis Summary for All Comment Columns:\")\n",
    "print(sentiment_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63808613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all comments into a single analysis\n",
    "# Combine all comment columns into one series for overall analysis\n",
    "all_comments = []\n",
    "for col in comment_columns:\n",
    "    non_empty = new[col].dropna()\n",
    "    non_empty = non_empty[non_empty.astype(str).str.strip() != '']\n",
    "    all_comments.extend(non_empty.tolist())\n",
    "\n",
    "print(f\"Total non-empty comments across all columns: {len(all_comments)}\")\n",
    "\n",
    "# Analyze overall sentiment\n",
    "overall_sentiments = [analyze_sentiment(comment) for comment in all_comments]\n",
    "overall_polarities = [s[0] for s in overall_sentiments if not np.isnan(s[0])]\n",
    "overall_subjectivities = [s[1] for s in overall_sentiments if not np.isnan(s[1])]\n",
    "overall_labels = [s[2] for s in overall_sentiments]\n",
    "\n",
    "print(f\"\\nOverall Sentiment Statistics:\")\n",
    "print(f\"Average Polarity: {np.mean(overall_polarities):.4f}\")\n",
    "print(f\"Average Subjectivity: {np.mean(overall_subjectivities):.4f}\")\n",
    "print(f\"\\nSentiment Distribution:\")\n",
    "print(f\"Positive: {overall_labels.count('positive')} ({overall_labels.count('positive')/len(overall_labels)*100:.2f}%)\")\n",
    "print(f\"Negative: {overall_labels.count('negative')} ({overall_labels.count('negative')/len(overall_labels)*100:.2f}%)\")\n",
    "print(f\"Neutral: {overall_labels.count('neutral')} ({overall_labels.count('neutral')/len(overall_labels)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comments dataframe for displaying top comments\n",
    "# Create a dataframe with comments and their sentiments\n",
    "comments_with_sentiment = []\n",
    "for col in comment_columns:\n",
    "    for idx in new.index:\n",
    "        comment = new.loc[idx, col]\n",
    "        if pd.notna(comment) and str(comment).strip() != '':\n",
    "            sentiment_col = f'{col}_sentiment'\n",
    "            polarity_col = f'{col}_polarity'\n",
    "            sentiment = new.loc[idx, sentiment_col]\n",
    "            polarity = new.loc[idx, polarity_col]\n",
    "            if pd.notna(sentiment) and pd.notna(polarity):\n",
    "                comments_with_sentiment.append({\n",
    "                    'comment': str(comment),\n",
    "                    'sentiment': sentiment,\n",
    "                    'polarity': polarity,\n",
    "                    'column': col\n",
    "                })\n",
    "\n",
    "comments_df = pd.DataFrame(comments_with_sentiment)\n",
    "num_samples = 15\n",
    "print(f\"Prepared {len(comments_df)} comments with sentiment analysis for display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Top Positive Comments\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP {num_samples} POSITIVE COMMENTS (Highest Polarity)\")\n",
    "print(\"=\"*80)\n",
    "positive_comments = comments_df[comments_df['sentiment'] == 'positive'].sort_values('polarity', ascending=False)\n",
    "if len(positive_comments) > 0:\n",
    "    for i, (idx, row) in enumerate(positive_comments.head(num_samples).iterrows(), 1):\n",
    "        print(f\"\\n[{i}] Polarity: {row['polarity']:.4f} | Column: {row['column']}\")\n",
    "        print(f\"Comment: {row['comment'][:600]}\")  # Limit to 600 chars\n",
    "        if len(row['comment']) > 600:\n",
    "            print(\"... (truncated)\")\n",
    "else:\n",
    "    print(\"No positive comments found.\")\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56320139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Top Negative Comments\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP {num_samples} NEGATIVE COMMENTS (Lowest Polarity)\")\n",
    "print(\"=\"*80)\n",
    "negative_comments = comments_df[comments_df['sentiment'] == 'negative'].sort_values('polarity', ascending=True)\n",
    "if len(negative_comments) > 0:\n",
    "    for i, (idx, row) in enumerate(negative_comments.head(num_samples).iterrows(), 1):\n",
    "        print(f\"\\n[{i}] Polarity: {row['polarity']:.4f} | Column: {row['column']}\")\n",
    "        print(f\"Comment: {row['comment'][:600]}\")  # Limit to 600 chars\n",
    "        if len(row['comment']) > 600:\n",
    "            print(\"... (truncated)\")\n",
    "else:\n",
    "    print(\"No negative comments found.\")\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Top Neutral Comments\n",
    "print(\"=\"*80)\n",
    "print(f\"TOP {num_samples} NEUTRAL COMMENTS (Closest to Zero)\")\n",
    "print(\"=\"*80)\n",
    "neutral_comments = comments_df[comments_df['sentiment'] == 'neutral'].copy()\n",
    "neutral_comments['abs_polarity'] = neutral_comments['polarity'].abs()\n",
    "neutral_comments = neutral_comments.sort_values('abs_polarity', ascending=True)\n",
    "if len(neutral_comments) > 0:\n",
    "    for i, (idx, row) in enumerate(neutral_comments.head(num_samples).iterrows(), 1):\n",
    "        print(f\"\\n[{i}] Polarity: {row['polarity']:.4f} | Column: {row['column']}\")\n",
    "        print(f\"Comment: {row['comment'][:600]}\")  # Limit to 600 chars\n",
    "        if len(row['comment']) > 600:\n",
    "            print(\"... (truncated)\")\n",
    "else:\n",
    "    print(\"No neutral comments found.\")\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More detailed sample comments with better formatting\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED TOP COMMENTS BY SENTIMENT CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Number of top samples to display per category\n",
    "num_samples_detailed = 15\n",
    "\n",
    "# Function to get top comments efficiently across all columns\n",
    "def get_top_comments(sentiment_type, num_samples=15):\n",
    "    \"\"\"Get top comments of a specific sentiment type across all columns\"\"\"\n",
    "    samples = []\n",
    "    for col in comment_columns:\n",
    "        sentiment_col = f'{col}_sentiment'\n",
    "        polarity_col = f'{col}_polarity'\n",
    "        \n",
    "        # Get all matching comments from this column\n",
    "        mask = (new[sentiment_col] == sentiment_type) & new[col].notna()\n",
    "        matching_data = new[mask][[col, polarity_col]].copy()\n",
    "        \n",
    "        for idx, row in matching_data.iterrows():\n",
    "            comment = str(row[col]).strip()\n",
    "            if comment and comment != '':\n",
    "                polarity = row[polarity_col]\n",
    "                if pd.notna(polarity):\n",
    "                    samples.append({\n",
    "                        'comment': comment,\n",
    "                        'polarity': polarity,\n",
    "                        'column': col,\n",
    "                        'index': idx\n",
    "                    })\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    if len(samples) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    samples_df = pd.DataFrame(samples)\n",
    "    \n",
    "    # Sort based on sentiment type\n",
    "    if sentiment_type == 'positive':\n",
    "        # Sort by highest polarity (most positive first)\n",
    "        samples_df = samples_df.sort_values('polarity', ascending=False)\n",
    "    elif sentiment_type == 'negative':\n",
    "        # Sort by lowest polarity (most negative first)\n",
    "        samples_df = samples_df.sort_values('polarity', ascending=True)\n",
    "    else:  # neutral\n",
    "        # Sort by absolute polarity (closest to zero first)\n",
    "        samples_df['abs_polarity'] = samples_df['polarity'].abs()\n",
    "        samples_df = samples_df.sort_values('abs_polarity', ascending=True)\n",
    "    \n",
    "    return samples_df.head(num_samples)\n",
    "\n",
    "# Get and display positive comments\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"TOP {num_samples_detailed} POSITIVE COMMENTS (Highest Polarity)\")\n",
    "print(\"-\"*80)\n",
    "pos_samples = get_top_comments('positive', num_samples_detailed)\n",
    "if len(pos_samples) > 0:\n",
    "    for i, (idx, row) in enumerate(pos_samples.iterrows(), 1):\n",
    "        print(f\"\\n[Sample {i}]\")\n",
    "        print(f\"  Polarity Score: {row['polarity']:.4f}\")\n",
    "        print(f\"  Source Column: {row['column']}\")\n",
    "        print(f\"  Comment:\")\n",
    "        # Print comment with proper word wrapping\n",
    "        comment_text = row['comment']\n",
    "        if len(comment_text) > 800:\n",
    "            print(f\"  {comment_text[:800]}...\")\n",
    "        else:\n",
    "            print(f\"  {comment_text}\")\n",
    "else:\n",
    "    print(\"No positive comments found.\")\n",
    "\n",
    "# Get and display negative comments\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"TOP {num_samples_detailed} NEGATIVE COMMENTS (Lowest Polarity)\")\n",
    "print(\"-\"*80)\n",
    "neg_samples = get_top_comments('negative', num_samples_detailed)\n",
    "if len(neg_samples) > 0:\n",
    "    for i, (idx, row) in enumerate(neg_samples.iterrows(), 1):\n",
    "        print(f\"\\n[Sample {i}]\")\n",
    "        print(f\"  Polarity Score: {row['polarity']:.4f}\")\n",
    "        print(f\"  Source Column: {row['column']}\")\n",
    "        print(f\"  Comment:\")\n",
    "        comment_text = row['comment']\n",
    "        if len(comment_text) > 800:\n",
    "            print(f\"  {comment_text[:800]}...\")\n",
    "        else:\n",
    "            print(f\"  {comment_text}\")\n",
    "else:\n",
    "    print(\"No negative comments found.\")\n",
    "\n",
    "# Get and display neutral comments\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"TOP {num_samples_detailed} NEUTRAL COMMENTS (Closest to Zero Polarity)\")\n",
    "print(\"-\"*80)\n",
    "neu_samples = get_top_comments('neutral', num_samples_detailed)\n",
    "if len(neu_samples) > 0:\n",
    "    for i, (idx, row) in enumerate(neu_samples.iterrows(), 1):\n",
    "        print(f\"\\n[Sample {i}]\")\n",
    "        print(f\"  Polarity Score: {row['polarity']:.4f}\")\n",
    "        print(f\"  Source Column: {row['column']}\")\n",
    "        print(f\"  Comment:\")\n",
    "        comment_text = row['comment']\n",
    "        if len(comment_text) > 800:\n",
    "            print(f\"  {comment_text[:800]}...\")\n",
    "        else:\n",
    "            print(f\"  {comment_text}\")\n",
    "else:\n",
    "    print(\"No neutral comments found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afe3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Overall Sentiment Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts = pd.Series(overall_labels).value_counts()\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
    "sentiment_counts.plot(kind='bar', color=[colors.get(x, 'blue') for x in sentiment_counts.index])\n",
    "plt.title('Overall Sentiment Distribution Across All Comments')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Polarity Distribution Histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(overall_polarities, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Neutral')\n",
    "plt.title('Distribution of Sentiment Polarity Scores')\n",
    "plt.xlabel('Polarity Score (Negative to Positive)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Average Polarity by Comment Column (Top 20 columns with most comments)\n",
    "top_columns = sentiment_summary.head(20)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(range(len(top_columns)), top_columns['avg_polarity'], \n",
    "         color=['green' if x > 0 else 'red' if x < 0 else 'gray' for x in top_columns['avg_polarity']])\n",
    "plt.yticks(range(len(top_columns)), top_columns.index)\n",
    "plt.xlabel('Average Polarity Score')\n",
    "plt.title('Average Sentiment Polarity by Comment Column (Top 20 by Comment Count)')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Sentiment Distribution by Comment Column (Top 10)\n",
    "top_10_columns = sentiment_summary.head(10).index\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define color mapping for sentiment labels\n",
    "sentiment_colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
    "\n",
    "for idx, col in enumerate(top_10_columns):\n",
    "    sentiment_col = f'{col}_sentiment'\n",
    "    sentiment_dist = new[sentiment_col].value_counts()\n",
    "    colors = [sentiment_colors.get(x, 'blue') for x in sentiment_dist.index]\n",
    "    sentiment_dist.plot(kind='bar', ax=axes[idx], color=colors, rot=45)\n",
    "    axes[idx].set_title(f'{col}\\n({sentiment_summary.loc[col, \"non_empty_comments\"]} comments)')\n",
    "    axes[idx].set_xlabel('')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('Sentiment Distribution for Top 10 Comment Columns', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ad0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Polarity vs Subjectivity Scatter Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Sample for better visualization if too many points\n",
    "if len(overall_polarities) > 5000:\n",
    "    sample_indices = np.random.choice(len(overall_polarities), 5000, replace=False)\n",
    "    sample_polarities = [overall_polarities[i] for i in sample_indices]\n",
    "    sample_subjectivities = [overall_subjectivities[i] for i in sample_indices]\n",
    "else:\n",
    "    sample_polarities = overall_polarities\n",
    "    sample_subjectivities = overall_subjectivities\n",
    "\n",
    "plt.scatter(sample_polarities, sample_subjectivities, alpha=0.5, s=10)\n",
    "plt.xlabel('Polarity (Negative to Positive)')\n",
    "plt.ylabel('Subjectivity (Objective to Subjective)')\n",
    "plt.title('Sentiment Analysis: Polarity vs Subjectivity')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cf1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Summary Report\n",
    "print(\"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Comment Columns Analyzed: {len(comment_columns)}\")\n",
    "print(f\"Total Non-Empty Comments: {len(all_comments)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"OVERALL SENTIMENT METRICS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Average Polarity: {np.mean(overall_polarities):.4f}\")\n",
    "print(f\"  - Range: [{np.min(overall_polarities):.4f}, {np.max(overall_polarities):.4f}]\")\n",
    "print(f\"  - Standard Deviation: {np.std(overall_polarities):.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Subjectivity: {np.mean(overall_subjectivities):.4f}\")\n",
    "print(f\"  - Range: [{np.min(overall_subjectivities):.4f}, {np.max(overall_subjectivities):.4f}]\")\n",
    "print(f\"  - Standard Deviation: {np.std(overall_subjectivities):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SENTIMENT LABEL DISTRIBUTION\")\n",
    "print(\"-\"*80)\n",
    "label_counts = pd.Series(overall_labels).value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(overall_labels)) * 100\n",
    "    print(f\"{label.capitalize()}: {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TOP 10 COMMENT COLUMNS BY COMMENT COUNT\")\n",
    "print(\"-\"*80)\n",
    "top_10_summary = sentiment_summary.head(10)\n",
    "for col in top_10_summary.index:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  - Non-empty comments: {top_10_summary.loc[col, 'non_empty_comments']}\")\n",
    "    print(f\"  - Average polarity: {top_10_summary.loc[col, 'avg_polarity']:.4f}\")\n",
    "    print(f\"  - Positive: {top_10_summary.loc[col, 'positive_count']}, \"\n",
    "          f\"Negative: {top_10_summary.loc[col, 'negative_count']}, \"\n",
    "          f\"Neutral: {top_10_summary.loc[col, 'neutral_count']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export sentiment summary to CSV\n",
    "# Ensure results directory exists\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "csv_path = os.path.join(results_dir, 'sentiment_analysis_summary.csv')\n",
    "sentiment_summary.to_csv(csv_path)\n",
    "print(f\"Sentiment analysis summary exported to '{csv_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4403031,
     "sourceId": 7561693,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30618,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "r",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.341814,
   "end_time": "2024-02-05T12:43:48.306120",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-05T12:43:30.964306",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
